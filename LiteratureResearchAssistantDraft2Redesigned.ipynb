{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f24780f-8fde-4cc2-93ea-3c78942057f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Literature Research Assistant – Architecture Notes\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "My goal with this project is to build a multi agent workflow that behaves like\n",
    "a lightweight, semi automated literature review assistant.\n",
    "\n",
    "Highlevel pipeline (as I currently have it):\n",
    "\n",
    "1. Intent Agent\n",
    "   - Takes my messy natural language question and turns it into a structured\n",
    "     ResearchIntent (Pydantic model).\n",
    "   - This is where I make all constraints explicit: topic, population, dates,\n",
    "     study types, risk factors, etc.\n",
    "   - Design choice: I keep this \"conservative\" so the agent does NOT invent\n",
    "     constraints I didn't ask for.\n",
    "\n",
    "2. Semantic Scholar Search Agent\n",
    "   - Translates the ResearchIntent into a Semantic Scholar query string.\n",
    "   - Calls the Semantic Scholar search API (not the graph API) to fetch a first\n",
    "     batch of candidate papers.\n",
    "   - Design choice: I keep the query fairly simple (topic + a few extras) to\n",
    "     avoid overconstraining and missing relevant papers.\n",
    "\n",
    "3. Filter Agent (Screening)\n",
    "   - Uses an LLM to screen titles/abstracts against my intent.\n",
    "   - Outputs two lists: kept vs rejected papers.\n",
    "   - Design choice: this is the quick way to emulate initial abstract\n",
    "     screening in a scoping review.\n",
    "\n",
    "4. Quality Ranking Agent (Academic Graph, no LLM)\n",
    "   - Uses Semantic Scholar's Academic Graph API (paper/batch endpoint) to\n",
    "     fetch metadata like citations, influential citations, venue, year, etc.\n",
    "   - Computes a quality_score in range (0,1) plus human readable reasons.\n",
    "   - Design choice: I rely on metadata here instead of asking\n",
    "     the LLM to \"guess quality\", so this is more reproducible and interpretable.\n",
    "\n",
    "5. Info Extraction Agent\n",
    "   - Guided by intent.target_entities, extracts specific entities from the\n",
    "     top ranked papers (e.g., genes, pathways, risk factors).\n",
    "   - Returns a list of ExtractedInfo objects with:\n",
    "       - paper_id\n",
    "       - entities: {category -> (entities)}\n",
    "       - entity_notes: {entity -> note}\n",
    "   - Design choice: the model is constrained to categories I define \n",
    "     instead of making up arbitrary ones.\n",
    "\n",
    "6. Trend Analysis Agent\n",
    "   - Looks at extracted entities across all papers and synthesizes:\n",
    "       - consensus_findings: where multiple papers agree\n",
    "       - conflicts: where papers disagree\n",
    "   - Design choice: this is my meta-analysis lite step, it operates only on\n",
    "     the structured extraction output, not on full texts\n",
    "\n",
    "7. Human Checkpoint Agent\n",
    "   - CLI-based loop where I/(the user) can:\n",
    "       - inspect the current intent in JSON form\n",
    "       - optionally edit fields (topic, population, dates, etc.)\n",
    "       - stop the pipeline if something is way off\n",
    "   - Design choice: this keeps me in the loop and prevents the system from\n",
    "     running off a misinterpreted question.\n",
    "\n",
    "8. Output Formatting Agent\n",
    "   - Takes the final state (intent, consensus, conflicts, ranked papers, my\n",
    "     checkpoint notes) and writes a Markdown mini report.\n",
    "   - Design choice: I use this primarily for structure and wording.\n",
    "\n",
    "Key data structures I rely on:\n",
    "------------------------------\n",
    "- ResearchIntent (Pydantic model):\n",
    "    My structured representation of the question + constraints. The most\n",
    "    important field I customized is:\n",
    "      - target_entities: Dict[str, List[str]]\n",
    "        Example:\n",
    "          {\n",
    "             \"genes\": [\"BRCA1\", \"BRCA2\"],\n",
    "             \"risk_factors\": [\"family history\", \"BMI\"]\n",
    "          }\n",
    "\n",
    "- ResearchState (TypedDict):\n",
    "    The \"shared memory\" object the supervisor passes between agents.\n",
    "    It carries user_query, intent, raw/filtered/ranked papers, extracted_info,\n",
    "    consensus/conflicts, checkpoint_notes, and formatted_output.\n",
    "\n",
    "Philosophy / guiding principles:\n",
    "--------------------------------\n",
    "- Be conservative about what the agents are allowed to assume.\n",
    "- Make constraints explicit in the ResearchIntent instead of hiding them in\n",
    "  prompts.\n",
    "- Separate responsibilities clearly:\n",
    "     search vs screening vs quality vs extraction vs synthesis vs formatting\n",
    "- Keep a human checkpoint before the final write up.\n",
    "- Use the Academic Graph for quality instead of asking the LLM to\n",
    "  make them up.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ee50a705-2f31-4065-8300-5fd972d17f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting API keys into the notebook's environment variables \n",
    "# so that the rest of the code can read them\n",
    "\n",
    "import os\n",
    "os.environ[\"SEMANTIC_SCHOLAR_API_KEY\"] = \"insert\" \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"insert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fa9f8153-121b-4e7f-ace2-1f18ba29a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create core_setup in notebook module \n",
    "# Conceptually: I'm pretending I have a core_setup.py file, but I'm\n",
    "# building it dynamically inside this notebook so I can do the following:\n",
    "## from core_setup import llm, ResearchIntent, ResearchState, ...\n",
    "# in later cells without copy/pasting model definitions everywhere\n",
    "\n",
    "import types, sys\n",
    "core_setup = types.ModuleType(\"core_setup\")\n",
    "sys.modules[\"core_setup\"] = core_setup\n",
    "\n",
    "# Put all shared types, state structures, and LLM here so all agents\n",
    "# share a single source of truth.\n",
    "from typing import TypedDict, List, Dict, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ----------------------------- LLM ------------------------------------\n",
    "## Here I'm defining a single base LLM instance that all agents can use.\n",
    "## This makes it easier to change the model/temperature in one place.\n",
    "core_setup.llm = ChatOpenAI(\n",
    "    model=\"gpt-5-mini\",  # lightweight model for agent orchestration\n",
    "    temperature=0.1,     # slightly creative but still controlled\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------- PAPER + EXTRACTED TYPES -----------------------\n",
    "## Paper: the minimal structure I use throughout the pipeline to\n",
    "## represent an article returned from Semantic Scholar.\n",
    "class Paper(TypedDict, total=False):\n",
    "    id: str                   # Semantic Scholar paperId\n",
    "    title: str\n",
    "    abstract: str\n",
    "    year: int\n",
    "    venue: str\n",
    "    authors: List[str]\n",
    "    citation_count: int       # citationCount from S2\n",
    "    url: Optional[str]\n",
    "\n",
    "core_setup.Paper = Paper\n",
    "\n",
    "\n",
    "# RankedPaper: same as Paper but with an added quality score + reasons\n",
    "# after passing through my quality ranking agent.\n",
    "class RankedPaper(Paper, total=False):\n",
    "    quality_score: float            # numeric quality score in (0,1)\n",
    "    quality_reasons: List[str]      # human readable explanations\n",
    "\n",
    "core_setup.RankedPaper = RankedPaper\n",
    "\n",
    "\n",
    "# ExtractedInfo: what my info extraction agent pulls from each paper.\n",
    "# I designed this to be generic so it can handle different entity types.\n",
    "class ExtractedInfo(TypedDict, total=False):\n",
    "    paper_id: str\n",
    "    # generic mapping: category -> list of entities\n",
    "    # (e.g., \"genes\": (\"BRCA1\", \"BRCA2\"))\n",
    "    entities: Dict[str, List[str]]\n",
    "    # entity_notes can hold short human readable notes per entity\n",
    "    # (e.g., \"BRCA1\": \"consistently associated with higher risk\")\n",
    "    entity_notes: Dict[str, str]\n",
    "\n",
    "core_setup.ExtractedInfo = ExtractedInfo\n",
    "\n",
    "\n",
    "# ---------------------- CONSENSUS / CONFLICT TYPES ------------------\n",
    "## These are higher level findings aggregated across papers.\n",
    "\n",
    "class ConsensusFinding(TypedDict, total=False):\n",
    "    entity: str                      # e.g., \"BRCA1\"\n",
    "    evidence_support_papers: List[str]  # paper_ids backing the finding\n",
    "    summary: str                     # narrative summary of consensus\n",
    "\n",
    "core_setup.ConsensusFinding = ConsensusFinding\n",
    "\n",
    "\n",
    "class ConflictFinding(TypedDict, total=False):\n",
    "    entity: str\n",
    "    supporting_papers: List[str]      # papers that support the effect\n",
    "    contradicting_papers: List[str]   # papers that contradict it\n",
    "    notes: str                        # how I interpret the conflict\n",
    "\n",
    "core_setup.ConflictFinding = ConflictFinding\n",
    "\n",
    "\n",
    "# ------------------------ INTENT MODEL -------------------------------\n",
    "## This is my structured representation of a user’s research question.\n",
    "## I use Pydantic so LangChain can parse LLM output into this schema\n",
    "## and I get handy .model_dump() / validation.\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ResearchIntent(BaseModel):\n",
    "    # A concise natural language question, e.g.,\n",
    "    # \"What genetic and lifestyle risk factors influence breast cancer\n",
    "    #  risk in young women?\"\n",
    "    high_level_question: str = \"\"\n",
    "\n",
    "    # Overall topic (shorter than high_level_question). This is often\n",
    "    # what I use as the core of the Semantic Scholar search query.\n",
    "    topic: str = \"\"\n",
    "\n",
    "    # Optional population constraint, I only fill this if the user\n",
    "    # explicitly mentions it (e.g., \"women under 40\", \"adults with SLE\").\n",
    "    population: Optional[str] = None\n",
    "\n",
    "    # Optional date range (e.g., \"2015-2025\", \"last 5 years\").\n",
    "    date_range: Optional[str] = \"\"\n",
    "\n",
    "    # Outcomes the user cares about (incidence, mortality, biomarkers…)\n",
    "    outcomes: Optional[List[str]] = []\n",
    "\n",
    "    # Requested study designs, if any (cohort, RCTs, meta-analyses, etc.).\n",
    "    study_types: Optional[List[str]] = []\n",
    "\n",
    "    # Terms that must appear in search (genes, pathways, technical keywords).\n",
    "    must_include_terms: Optional[List[str]] = []\n",
    "\n",
    "    # Terms to exclude (e.g., \"mouse\", \"in vitro\", \"case reports\").\n",
    "    exclude_terms: Optional[List[str]] = []\n",
    "\n",
    "    # Free text notes from me/user to the agent/downstream steps.\n",
    "    notes_for_supervisor: Optional[str] = \"\"\n",
    "\n",
    "    # If I want a very specific boolean search string, I can override\n",
    "    # the auto generated query here. Otherwise this stays empty.\n",
    "    semantic_scholar_query: Optional[str] = \"\"\n",
    "\n",
    "    # Depth of review: quick, medium, or deep.\n",
    "    depth: str = \"medium\"\n",
    "\n",
    "    # Important: this is a dictionary, not a list.\n",
    "    # Conceptually: this is what I want the info extraction agent to\n",
    "    # look for in the papers. Example structure:\n",
    "    #\n",
    "    #   {\n",
    "    #       \"genes\": (\"BRCA1\", \"BRCA2\"),\n",
    "    #       \"risk_factors\": (\"family history\", \"BMI\")\n",
    "    #   }\n",
    "    #\n",
    "    # Keys are categories, values are example entities.\n",
    "    target_entities: Dict[str, List[str]] = {}\n",
    "\n",
    "core_setup.ResearchIntent = ResearchIntent\n",
    "\n",
    "\n",
    "# ------------------------- RESEARCH STATE ----------------------------\n",
    "## This is the global state my supervisor passes between agents.\n",
    "class ResearchState(TypedDict, total=False):\n",
    "    user_query: str\n",
    "    intent: ResearchIntent\n",
    "    raw_papers: List[Paper]              # direct Semantic Scholar hits\n",
    "    filtered_papers: List[Paper]         # after my screening agent\n",
    "    ranked_papers: List[RankedPaper]     # quality ranked subset\n",
    "    extracted_info: List[ExtractedInfo]  # entities per paper\n",
    "    consensus_findings: List[ConsensusFinding]\n",
    "    conflicts: List[ConflictFinding]\n",
    "    checkpoint_notes: Optional[str]      # notes from human checkpoint\n",
    "    formatted_output: str                # final Markdown report\n",
    "\n",
    "core_setup.ResearchState = ResearchState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "42c7c3ad-a5ef-4d07-a4aa-7b1bcc9833fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# INTENT AGENT – DESIGN NOTES\n",
    "# ----------------------------\n",
    "# This agent is my beginning of the pipeline.\n",
    "#\n",
    "# From my perspective:\n",
    "# - I usually start with a messy question like:\n",
    "#     \"What are the leading genetic and lifestyle risk factors for breast cancer\n",
    "#      in young adults?\"\n",
    "# - I want the system to pin down that question into structured fields:\n",
    "#     topic, population, outcomes, date_range, study_types, etc.\n",
    "# - I also want to tell downstream agents what to extract via target_entities,\n",
    "#   but in a controlled way.\n",
    "#\n",
    "# Key design decisions:\n",
    "# - I treat target_entities as a DICT (category -> examples), not a flat list.\n",
    "#   This gives me more semantic control, e.g.:\n",
    "#       {\"genes\": [\"BRCA1\", \"BRCA2\"], \"risk_factors\": [\"BMI\", \"family history\"]}\n",
    "# - The prompt insists on being conservative:\n",
    "#     - If the user didn't clearly specify something, we leave it blank or default.\n",
    "#     - This reduces the risk that the system \"imagines\" constraints.\n",
    "# - I parse the LLM output into a Pydantic ResearchIntent so that:\n",
    "#     - I get validation and a consistent shape of data\n",
    "#     - Downstream code doesn't have to worry whether it's a dict or model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ccd9c211-ca3a-482b-9a37-2c802582e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent Agent\n",
    "\n",
    "\n",
    "# Here I'm translating a natural language user query into my\n",
    "# structured ResearchIntent object using the LLM\n",
    "\n",
    "from core_setup import llm, ResearchIntent, ResearchState\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "intent_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            # I’m explaining to the LLM exactly what I want it to do.\n",
    "            \"You are a research planner. Convert the user query into a structured ResearchIntent.\\n\"\n",
    "            \"Be conservative and explicit about constraints.\\n\\n\"\n",
    "            \"You MUST return ONLY valid JSON with the following keys EXACTLY matching the ResearchIntent schema:\\n\"\n",
    "            \"  high_level_question (str)\\n\"\n",
    "            \"  topic (str)\\n\"\n",
    "            \"  population (str or null)\\n\"\n",
    "            \"  date_range (str)\\n\"\n",
    "            \"  outcomes (list of str)\\n\"\n",
    "            \"  study_types (list of str)\\n\"\n",
    "            \"  must_include_terms (list of str)\\n\"\n",
    "            \"  exclude_terms (list of str)\\n\"\n",
    "            \"  notes_for_supervisor (str)\\n\"\n",
    "            \"  semantic_scholar_query (str)\\n\"\n",
    "            \"  depth (str: 'quick' | 'medium' | 'deep')\\n\"\n",
    "            \"  target_entities (object/dict)\\n\\n\"\n",
    "            \"IMPORTANT — target_entities must be a DICTIONARY, not a list.\\n\"\n",
    "            \"Structure target_entities as a JSON object where:\\n\"\n",
    "            \"- Keys are category names (e.g. 'genes', 'risk_factors', 'biomarkers').\\n\"\n",
    "            \"- Values are lists of example entities or synonyms for that category (e.g. ['BRCA1', 'BRCA2']).\\n\\n\"\n",
    "            \"Rules for target_entities:\\n\"\n",
    "            \"- Include ONLY categories clearly implied by the user.\\n\"\n",
    "            \"- If the user mentions genes, biomarkers, pathways, risk factors, etc., use those as category names.\\n\"\n",
    "            \"- Do NOT guess extra categories.\\n\"\n",
    "            \"- If nothing is implied, return an empty object (an empty JSON dictionary).\\n\\n\"\n",
    "            \"General field rules:\\n\"\n",
    "            \"- high_level_question should summarize WHAT the user wants answered.\\n\"\n",
    "            \"- topic is the main subject (disease, exposure, outcome).\\n\"\n",
    "            \"- population only if user explicitly defines it.\\n\"\n",
    "            \"- date_range only if explicitly mentioned.\\n\"\n",
    "            \"- must_include_terms and exclude_terms only if clearly present.\\n\"\n",
    "            \"- semantic_scholar_query only if user provides a boolean-style literal query.\\n\"\n",
    "            \"- depth defaults to 'medium' unless user indicates otherwise.\\n\"\n",
    "            \"- NEVER invent fields or constraints.\\n\"\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            # I’m providing the raw user query as {user_query}.\n",
    "            \"User query:\\n{user_query}\\n\\n\"\n",
    "            \"Return the ResearchIntent JSON object now.\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Let the parser return a plain dict, I’ll wrap it into ResearchIntent myself.\n",
    "intent_parser = JsonOutputParser(pydantic_object=None)\n",
    "\n",
    "def run_intent_agent(user_query: str) -> ResearchIntent:\n",
    "    \"\"\"\n",
    "    Take the raw user query string and return a fully-typed ResearchIntent.\n",
    "    \"\"\"\n",
    "    # Chain: prompt -> LLM -> JSON parsing.\n",
    "    chain = intent_prompt | llm | intent_parser\n",
    "    raw = chain.invoke({\"user_query\": user_query})\n",
    "\n",
    "    # Convert dict -> ResearchIntent so downstream code always sees\n",
    "    # a Pydantic object, not arbitrary dict structures.\n",
    "    if isinstance(raw, dict):\n",
    "        return ResearchIntent(**raw)\n",
    "\n",
    "    # Fallback: if langchain returns a Pydantic-like object, use .model_dump()\n",
    "    return ResearchIntent(**raw.model_dump())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e51b5-5c8a-4098-91f9-20a4ec2a2aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ddc11b66-f385-4783-9b57-3afc631bbf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# SEMANTIC SCHOLAR SEARCH – DESIGN NOTES\n",
    "# ---------------------------------------\n",
    "# This agent converts my structured ResearchIntent into a search query\n",
    "# for the Semantic Scholar REST API.\n",
    "#\n",
    "# From my perspective:\n",
    "# - I don't want the search query to be overly complex or fragile.\n",
    "# - I want \"topic\" to be the main driver, with a few helper terms:\n",
    "#     population, must_include_terms, and (optionally) target_entities keys.\n",
    "#\n",
    "# Design choices:\n",
    "# - If I explicitly set `semantic_scholar_query` in the intent, that wins.\n",
    "#   This gives me a manual override when I'm being picky.\n",
    "# - If not, I build a query string like:\n",
    "#       \"<topic or HLQ> <population?> <first must_include terms> <few entity categories>\"\n",
    "# - I deliberately avoid injecting every possible detail into the query,\n",
    "#   because that can reduce recall and miss relevant papers.\n",
    "# - The returned structure is normalized into my Paper TypedDict to keep\n",
    "#   downstream agents model agnostic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1d81c880-c811-4fb0-85d2-bef37668ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Scholar Agent\n",
    "\n",
    "\n",
    "# This agent is responsible for building a search query from my\n",
    "# ResearchIntent and calling the Semantic Scholar search API.\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from typing import List\n",
    "from core_setup import Paper, ResearchIntent\n",
    "\n",
    "SEMANTIC_SCHOLAR_API_KEY = os.environ.get(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "\n",
    "BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "\n",
    "\n",
    "def build_search_query(intent) -> str:\n",
    "    \"\"\"\n",
    "    Build a Semantic Scholar search query from a ResearchIntent.\n",
    "\n",
    "    Strategy:\n",
    "      1) If semantic_scholar_query is set, use that directly.\n",
    "      2) Otherwise, use topic as the core, and only add a few extra terms\n",
    "         (population, first 1–2 must_include_terms, optionally some\n",
    "         target_entity categories).\n",
    "    \"\"\"\n",
    "    # Handle both Pydantic model and dict (for flexibility).\n",
    "    if hasattr(intent, \"model_dump\"):\n",
    "        data = intent.model_dump()\n",
    "    else:\n",
    "        data = dict(intent)\n",
    "\n",
    "    # 1) Explicit query wins, this lets me override everything manually.\n",
    "    explicit_q = (data.get(\"semantic_scholar_query\") or \"\").strip()\n",
    "    if explicit_q:\n",
    "        return explicit_q\n",
    "\n",
    "    topic = (data.get(\"topic\") or \"\").strip()\n",
    "    hlq = (data.get(\"high_level_question\") or \"\").strip()\n",
    "    pop = (data.get(\"population\") or \"\").strip()\n",
    "    must_terms = [t.strip() for t in (data.get(\"must_include_terms\") or []) if t.strip()]\n",
    "\n",
    "    # Base: topic or high level question, if neither is present, I fall\n",
    "    # back to a generic phrase so the query is never empty.\n",
    "    if topic:\n",
    "        base = topic\n",
    "    elif hlq:\n",
    "        base = hlq\n",
    "    else:\n",
    "        base = \"cancer risk\"  # generic fallback if everything is blank\n",
    "\n",
    "    extras = []\n",
    "\n",
    "    # If a population is specified, I bias the query towards that group.\n",
    "    if pop:\n",
    "        extras.append(pop)\n",
    "\n",
    "    # I only add the first 1-2 must include terms so I don't over constrain.\n",
    "    extras.extend(must_terms[:2])\n",
    "\n",
    "    # OPTIONAL: add a few target_entity category names as soft context\n",
    "    raw_targets = data.get(\"target_entities\") or {}\n",
    "    target_categories: list[str] = []\n",
    "    if isinstance(raw_targets, dict):\n",
    "        # With the dict, I just use the keys (e.g., \"genes\").\n",
    "        target_categories = list(raw_targets.keys())[:3]\n",
    "    elif isinstance(raw_targets, list):\n",
    "        # Backwards compatibility in case I ever pass a list.\n",
    "        target_categories = raw_targets[:3]\n",
    "\n",
    "    extras.extend(target_categories)\n",
    "\n",
    "    # Join everything into a single query string.\n",
    "    query = \" \".join([base] + extras).strip()\n",
    "    return query\n",
    "\n",
    "\n",
    "def semantic_scholar_search(intent: ResearchIntent, limit: int = 50) -> List[Paper]:\n",
    "    \"\"\"\n",
    "    Call the Semantic Scholar search API using the built query and\n",
    "    convert results into my Paper schema.\n",
    "    \"\"\"\n",
    "    if SEMANTIC_SCHOLAR_API_KEY is None:\n",
    "        raise ValueError(\"Set SEMANTIC_SCHOLAR_API_KEY in environment variables.\")\n",
    "\n",
    "    query = build_search_query(intent)\n",
    "    print(f\"[DEBUG] Semantic Scholar query: {query!r}\")\n",
    "\n",
    "    fields = \"title,abstract,year,venue,authors,citationCount,url\"\n",
    "\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"limit\": limit,\n",
    "        \"fields\": fields,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"x-api-key\": SEMANTIC_SCHOLAR_API_KEY,\n",
    "    }\n",
    "\n",
    "    resp = requests.get(BASE_URL, headers=headers, params=params)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    papers: List[Paper] = []\n",
    "    for item in data.get(\"data\", []):\n",
    "        authors = [a[\"name\"] for a in item.get(\"authors\", [])]\n",
    "        paper: Paper = {\n",
    "            \"id\": item.get(\"paperId\", \"\"),\n",
    "            \"title\": item.get(\"title\") or \"\",\n",
    "            \"abstract\": item.get(\"abstract\") or \"\",\n",
    "            \"year\": item.get(\"year\") or 0,\n",
    "            \"venue\": item.get(\"venue\") or \"\",\n",
    "            \"authors\": authors,\n",
    "            \"citation_count\": item.get(\"citationCount\") or 0,\n",
    "            \"url\": item.get(\"url\") or \"\",\n",
    "        }\n",
    "        papers.append(paper)\n",
    "\n",
    "    return papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab817f-644b-47b7-9c03-80cc48fbdd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0c18df54-2a43-46e0-af98-7c2d5ba81810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# FILTER AGENT - DESIGN NOTES\n",
    "# ----------------------------\n",
    "# This LLM based agent emulates the first pass abstract screening step\n",
    "# in a real literature review.\n",
    "#\n",
    "# From my perspective:\n",
    "# - The search results might be noisy or off topic.\n",
    "# - I don't want to manually read all titles + abstracts just to get rid of\n",
    "#   obviously irrelevant papers.\n",
    "#\n",
    "# Design choices:\n",
    "# - I give the LLM the full ResearchIntent + the candidate papers and\n",
    "#   ask it to produce two lists: kept and rejected.\n",
    "# - I treat this as a high recall filter:\n",
    "#     - It's okay if it keeps a few weakly relevant papers.\n",
    "#     - It's worse if it drops papers that belong.\n",
    "# - I'm not using detailed quality criteria here, that's the job of the\n",
    "#   later quality ranking agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bd560437-c346-4a64-ad64-f0c3f528ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Agent\n",
    "\n",
    "\n",
    "# This agent screens raw Semantic Scholar hits to decide which papers\n",
    "# are relevant enough to keep.\n",
    "\n",
    "from typing import List, Dict\n",
    "from core_setup import ResearchIntent, Paper, ResearchState\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# I’m using a separate LLM instance here just to be explicit, but it\n",
    "# could also reuse core_setup.llm.\n",
    "llm_filter = ChatOpenAI(model=\"gpt-5-mini\", temperature=0.0)\n",
    "\n",
    "filter_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a paper screening assistant. \"\n",
    "            \"Given a list of papers and a research intent, decide which papers to KEEP or REJECT. \"\n",
    "            \"Return a purely JSON decision.\"\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Research intent (JSON):\\n{intent}\\n\\n\"\n",
    "            \"Papers (JSON list):\\n{papers}\\n\\n\"\n",
    "            \"Only keep papers that match the topic, population (if specified), and study types \"\n",
    "            \"and that are not excluded by the 'exclude' list.\\n\\n\"\n",
    "            \"Return ONLY JSON with exactly these keys:\\n\"\n",
    "            \"- 'kept': a list of paper IDs as strings (e.g., ['id1', 'id2']).\\n\"\n",
    "            \"- 'rejected': a list of paper IDs as strings.\\n\"\n",
    "            \"Do not return full paper objects, and do not include any other keys or text.\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "filter_parser = JsonOutputParser(pydantic_object=None)\n",
    "\n",
    "def run_filter_agent(intent: ResearchIntent, papers: List[Paper]) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Ask the LLM to decide which papers to keep vs reject, based on the\n",
    "    structured ResearchIntent.\n",
    "    \"\"\"\n",
    "    chain = filter_prompt | llm_filter | filter_parser\n",
    "    return chain.invoke({\n",
    "        \"intent\": intent,\n",
    "        \"papers\": papers\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b234a2-68a0-434d-a82a-e991067960a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6250554b-5296-4ba8-bc78-c87390dd44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# QUALITY RANKING AGENT (S2 ACADEMIC GRAPH) - DESIGN NOTES\n",
    "# ----------------------------------------------------------\n",
    "# This is my metadata-based quality assessment step.\n",
    "#\n",
    "# From my perspective:\n",
    "# - I want an objectiveish score that prefers:\n",
    "#     - more recent papers,\n",
    "#     - higher citation counts,\n",
    "#     - influential citations,\n",
    "#     - solid venues,\n",
    "#     - appropriate publication types (e.g., clinical trials > case reports).\n",
    "# - I don't want the LLM making up random quality scores.\n",
    "#\n",
    "# Design choices:\n",
    "# - I call the Semantic Scholar Academic Graph /paper/batch endpoint to\n",
    "#   fetch metadata in bulk for the filtered paper IDs.\n",
    "# - I normalize citation related metrics using logs so that:\n",
    "#     - Differences at low counts matter more than differences at crazy high counts.\n",
    "# - I compute a weighted score in (0,1) from:\n",
    "#     cit_score, influ_score, recency_score, venue_score, type_score.\n",
    "# - I always return reasons alongside the score so I have a transparent\n",
    "#   explanation when I later read the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b2b460bb-6a96-4afc-a2b9-16f13659e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality Ranking Agent (Semantic Scholar Academic Graph, no LLM)\n",
    "\n",
    "\n",
    "# Here I'm using purely heuristic scoring based on metadata from the\n",
    "# Semantic Scholar Academic Graph API (not the search API).\n",
    "# Goal: produce a list of RankedPaper objects with:\n",
    "## quality_score in (0,1)\n",
    "## quality_reasons: human readable breakdown of the factors\n",
    "\n",
    "import os\n",
    "import math\n",
    "import requests\n",
    "from typing import List, Dict\n",
    "from core_setup import Paper, RankedPaper\n",
    "\n",
    "S2_GRAPH_BASE = \"https://api.semanticscholar.org/graph/v1/paper\"\n",
    "\n",
    "\n",
    "def fetch_paper_details_batch(paper_ids: List[str], fields: str) -> Dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Call Semantic Scholar Graph API /paper/batch to get metadata for many paperIds.\n",
    "    Returns a dict: {paperId: metadata_dict}\n",
    "\n",
    "    I use this to pull citation counts, influential citations, year,\n",
    "    venue, publication types, etc. in one batch.\n",
    "    \"\"\"\n",
    "    if not paper_ids:\n",
    "        return {}\n",
    "\n",
    "    api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Set SEMANTIC_SCHOLAR_API_KEY in environment variables.\")\n",
    "\n",
    "    url = f\"{S2_GRAPH_BASE}/batch\"\n",
    "    headers = {\n",
    "        \"x-api-key\": api_key,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"ids\": paper_ids,\n",
    "        \"fields\": fields,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(url, headers=headers, json=payload)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    details_by_id: Dict[str, dict] = {}\n",
    "    for item in data:\n",
    "        pid = item.get(\"paperId\")\n",
    "        if pid:\n",
    "            details_by_id[pid] = item\n",
    "    return details_by_id\n",
    "\n",
    "\n",
    "def compute_quality_score(\n",
    "    meta: dict,\n",
    "    year_min: int,\n",
    "    year_max: int,\n",
    "    max_citations: int,\n",
    ") -> (float, dict):\n",
    "    \"\"\"\n",
    "    Compute a quality score in (0,1) using simple heuristics:\n",
    "    - citation count\n",
    "    - influential citation count\n",
    "    - recency (relative to other papers in this set)\n",
    "    - venue (high impact journal vs unknown)\n",
    "    - publication type (clinical trial, journal article, etc.)\n",
    "\n",
    "    I also return a dict of factor contributions so I can later build\n",
    "    human readable explanations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract raw values with defaults\n",
    "    year = meta.get(\"year\") or 0\n",
    "    citations = meta.get(\"citationCount\") or 0\n",
    "    influ = meta.get(\"influentialCitationCount\") or 0\n",
    "    venue = (meta.get(\"venue\") or \"\").lower()\n",
    "    pub_types = meta.get(\"publicationTypes\") or []\n",
    "    fields_of_study = meta.get(\"fieldsOfStudy\") or []\n",
    "\n",
    "    # --- normalized citation count ---\n",
    "    if max_citations > 0:\n",
    "        cit_score = math.log(citations + 1) / math.log(max_citations + 1)\n",
    "    else:\n",
    "        cit_score = 0.0\n",
    "\n",
    "    # --- normalized influential citations ---\n",
    "    if max_citations > 0:\n",
    "        influ_score = math.log(influ + 1) / math.log(max_citations + 1)\n",
    "    else:\n",
    "        influ_score = 0.0\n",
    "\n",
    "    # --- recency score (relative within the set) ---\n",
    "    if year_max > year_min and year > 0:\n",
    "        recency_score = (year - year_min) / (year_max - year_min)\n",
    "        recency_score = max(0.0, min(1.0, recency_score))\n",
    "    else:\n",
    "        recency_score = 0.5  # default neutral if I can't compute it\n",
    "\n",
    "    # --- venue heuristics ---\n",
    "    high_impact_keywords = [\"nature\", \"cell\", \"science\", \"lancet\", \"nejm\", \"jama\"]\n",
    "    if any(k in venue for k in high_impact_keywords):\n",
    "        venue_score = 1.0\n",
    "    elif venue:\n",
    "        venue_score = 0.6\n",
    "    else:\n",
    "        venue_score = 0.4\n",
    "\n",
    "    # --- publication type heuristics ---\n",
    "    pub_types_lower = [p.lower() for p in pub_types]\n",
    "    if any(\"clinical trial\" in p or \"randomized\" in p for p in pub_types_lower):\n",
    "        type_score = 1.0\n",
    "    elif any(\"journalarticle\" in p or \"journal article\" in p for p in pub_types_lower):\n",
    "        type_score = 0.8\n",
    "    elif pub_types_lower:\n",
    "        type_score = 0.6\n",
    "    else:\n",
    "        type_score = 0.5\n",
    "\n",
    "    # I could add field of study heuristics here, for now I just pass\n",
    "    # them through in the factors dict.\n",
    "\n",
    "    # Combine into a single quality score.\n",
    "    # These weights are arbitrary but reflect my rough priorities.\n",
    "    score = (\n",
    "        0.35 * cit_score +\n",
    "        0.25 * influ_score +\n",
    "        0.2  * recency_score +\n",
    "        0.1  * venue_score +\n",
    "        0.1  * type_score\n",
    "    )\n",
    "\n",
    "    factors = {\n",
    "        \"cit_score\": cit_score,\n",
    "        \"influ_score\": influ_score,\n",
    "        \"recency_score\": recency_score,\n",
    "        \"venue_score\": venue_score,\n",
    "        \"type_score\": type_score,\n",
    "        \"year\": year,\n",
    "        \"citations\": citations,\n",
    "        \"influential_citations\": influ,\n",
    "        \"venue\": venue,\n",
    "        \"pub_types\": pub_types,\n",
    "        \"fields_of_study\": fields_of_study,\n",
    "    }\n",
    "\n",
    "    return score, factors\n",
    "\n",
    "\n",
    "def run_quality_ranking_agent(papers: List[Paper]) -> List[RankedPaper]:\n",
    "    \"\"\"\n",
    "    Take my filtered papers and enrich them with Academic Graph metadata\n",
    "    to produce quality ranked papers with explanations.\n",
    "    \"\"\"\n",
    "    if not papers:\n",
    "        return []\n",
    "\n",
    "    # IDs I’ll look up\n",
    "    ids = [p[\"id\"] for p in papers if p.get(\"id\")]\n",
    "    if not ids:\n",
    "        return []\n",
    "\n",
    "    fields = (\n",
    "        \"year,venue,citationCount,influentialCitationCount,\"\n",
    "        \"publicationTypes,fieldsOfStudy\"\n",
    "    )\n",
    "\n",
    "    # --- fetch details in batches to respect limits ---\n",
    "    details: Dict[str, dict] = {}\n",
    "    batch_size = 50\n",
    "    for i in range(0, len(ids), batch_size):\n",
    "        batch_ids = ids[i : i + batch_size]\n",
    "        batch_details = fetch_paper_details_batch(batch_ids, fields)\n",
    "        details.update(batch_details)\n",
    "\n",
    "    # --- aggregate for normalization (year range, max citations) ---\n",
    "    years = [meta.get(\"year\") for meta in details.values() if meta.get(\"year\")]\n",
    "    citations_list = [meta.get(\"citationCount\") or 0 for meta in details.values()]\n",
    "    year_min = min(years) if years else 0\n",
    "    year_max = max(years) if years else 0\n",
    "    max_citations = max(citations_list) if citations_list else 0\n",
    "\n",
    "    ranked: List[RankedPaper] = []\n",
    "    for p in papers:\n",
    "        pid = p.get(\"id\")\n",
    "        meta = details.get(pid, {})  # if missing, meta = {}\n",
    "\n",
    "        score, factors = compute_quality_score(meta, year_min, year_max, max_citations)\n",
    "\n",
    "        # Build human readable reasons for the score.\n",
    "        reasons: List[str] = []\n",
    "\n",
    "        reasons.append(f\"Overall quality score: {score:.2f} (0–1 scale).\")\n",
    "\n",
    "        year = factors[\"year\"]\n",
    "        if year:\n",
    "            reasons.append(f\"Publication year: {year} (relative recency score {factors['recency_score']:.2f}).\")\n",
    "\n",
    "        citations = factors[\"citations\"]\n",
    "        if citations:\n",
    "            reasons.append(f\"Citations: {citations} (normalized score {factors['cit_score']:.2f}).\")\n",
    "\n",
    "        influ = factors[\"influential_citations\"]\n",
    "        if influ:\n",
    "            reasons.append(f\"Influential citations: {influ} (normalized score {factors['influ_score']:.2f}).\")\n",
    "\n",
    "        venue = factors[\"venue\"]\n",
    "        if venue:\n",
    "            reasons.append(f\"Venue: {venue} (venue score {factors['venue_score']:.2f}).\")\n",
    "\n",
    "        pub_types = factors[\"pub_types\"]\n",
    "        if pub_types:\n",
    "            reasons.append(f\"Publication types: {', '.join(pub_types)} (type score {factors['type_score']:.2f}).\")\n",
    "\n",
    "        fields_of_study = factors[\"fields_of_study\"] or []\n",
    "        if fields_of_study:\n",
    "            reasons.append(f\"Fields of study: {', '.join(fields_of_study)}.\")\n",
    "\n",
    "        rp: RankedPaper = {\n",
    "            **p,\n",
    "            \"quality_score\": score,\n",
    "            \"quality_reasons\": reasons,\n",
    "        }\n",
    "        ranked.append(rp)\n",
    "\n",
    "    # Highest quality first\n",
    "    ranked.sort(key=lambda x: x[\"quality_score\"], reverse=True)\n",
    "    return ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ec3ba779-34d6-49b3-96e6-da273f554ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# INFO EXTRACTION AGENT - DESIGN NOTES\n",
    "# -------------------------------------\n",
    "# This step tries to answer: \"Given the top papers, what entities do\n",
    "# they mention that match what I care about?\"\n",
    "#\n",
    "# From my perspective:\n",
    "# - I care about things like genes, pathways, biomarkers, risk factors, etc.\n",
    "# - But I don't want a free for all where the model pulls out anything.\n",
    "#\n",
    "# Design choices:\n",
    "# - I use intent.target_entities as a DICTIONARY to specify:\n",
    "#     - which categories to focus on (keys),\n",
    "#     - which example entities/synonyms prime the model (values).\n",
    "# - The agent loops over the ranked papers and, for each one, outputs:\n",
    "#     paper_id, entities (by category), and entity_notes (small comments).\n",
    "# - Downstream trend analysis works purely on this structured layer,\n",
    "#   not on raw text, which keeps things more interpretable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3b4aaf2e-a5c4-41c1-a3a7-df95db32b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info Extraction Agent\n",
    "\n",
    "\n",
    "# This agent reads ranked papers + the structured intent and pulls out\n",
    "# specific entities (genes, pathways, biomarkers, etc.) that I care\n",
    "# about, according to intent.target_entities.\n",
    "\n",
    "from typing import List\n",
    "from core_setup import RankedPaper, ExtractedInfo, ResearchIntent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "llm_extract = ChatOpenAI(model=\"gpt-5-mini\", temperature=0.0)\n",
    "\n",
    "extract_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a biomedical information extraction assistant. \"\n",
    "            \"You will be given a research intent and a list of papers (title + abstract).\\n\\n\"\n",
    "            \"The intent.target_entities field is a dictionary mapping category names \"\n",
    "            \"(e.g. 'genes', 'risk_factors', 'biomarkers') to example entities or synonyms.\\n\\n\"\n",
    "            \"For each paper, you must output an object with:\\n\"\n",
    "            \"- paper_id (string)\\n\"\n",
    "            \"- entities (object mapping each target entity category to a list of strings actually mentioned in this paper)\\n\"\n",
    "            \"- entity_notes (object mapping specific entities to short notes, \"\n",
    "            \"such as direction of effect or risk interpretation).\\n\"\n",
    "            \"If a category has nothing relevant for a given paper, use an empty list for that category.\"\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Research intent (JSON):\\n{intent}\\n\\n\"\n",
    "            \"Papers (JSON list):\\n{papers}\\n\\n\"\n",
    "            \"Return ONLY a JSON list of objects with keys: paper_id, entities, entity_notes.\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "extract_parser = JsonOutputParser(pydantic_object=None)\n",
    "\n",
    "def run_info_extraction_agent(\n",
    "    intent: ResearchIntent,\n",
    "    ranked_papers: List[RankedPaper],\n",
    ") -> List[ExtractedInfo]:\n",
    "    \"\"\"\n",
    "    Use the LLM to extract entities of interest from the top ranked papers,\n",
    "    guided by the target_entities mapping in the ResearchIntent.\n",
    "    \"\"\"\n",
    "    # Convert Pydantic intent -> plain dict if needed.\n",
    "    if hasattr(intent, \"model_dump\"):\n",
    "        intent_json = intent.model_dump()\n",
    "    else:\n",
    "        intent_json = intent\n",
    "\n",
    "    chain = extract_prompt | llm_extract | extract_parser\n",
    "    return chain.invoke(\n",
    "        {\n",
    "            \"intent\": intent_json,\n",
    "            \"papers\": ranked_papers,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f3e54-a982-4621-a173-40e5211ffa73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e27c3-f740-46d3-a766-c82427d6b1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "715b7421-2a88-4a5e-84be-ff13b8ef7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# TREND ANALYSIS AGENT – DESIGN NOTES\n",
    "# -----------------------------------\n",
    "# Given all the extracted entities across papers, this agent tries to\n",
    "# answer: \"Where do papers agree?\" and \"Where do they disagree?\"\n",
    "#\n",
    "# From my perspective:\n",
    "# - This is my meta-analysis lite step, but I'm not doing stats here.\n",
    "# - I mainly want:\n",
    "#     - a list of entities with strong multi paper support (consensus),\n",
    "#     - a list of entities where evidence is mixed or conflicting.\n",
    "#\n",
    "# Design choices:\n",
    "# - I let the LLM operate only on the structured ExtractedInfo objects,\n",
    "#   not full abstracts, this focuses it on the curated signals instead of\n",
    "#   raw text noise.\n",
    "# - The output schema is:\n",
    "#     consensus_findings: ({entity, evidence_support_papers, summary})\n",
    "#     conflicts: [{entity, supporting_papers, contradicting_papers, notes}]\n",
    "# - This is what I show next to myself at the checkpoint to judge if the\n",
    "#   system is picking up the right story.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "75c641cd-bde9-4df4-953e-f5a9396fdd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend Analysis Agent\n",
    "\n",
    "\n",
    "# Here I take the extracted entities from all papers and ask the LLM\n",
    "# to synthesize consensus vs conflicts across the literature.\n",
    "\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "from core_setup import ExtractedInfo\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "llm_trend = ChatOpenAI(model=\"gpt-5-mini\", temperature=0.0)\n",
    "\n",
    "trend_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a meta-analysis assistant. \"\n",
    "            \"You will be given extracted entities from multiple papers.\\n\"\n",
    "            \"Each entry has 'paper_id', 'entities' (category -> list of strings), \"\n",
    "            \"and 'entity_notes'.\\n\\n\"\n",
    "            \"Your job is to:\\n\"\n",
    "            \"- Identify which specific entities (e.g., particular risk factors, genes, scores) \"\n",
    "            \"are supported by multiple papers.\\n\"\n",
    "            \"- Summarize the overall consensus about these entities.\\n\"\n",
    "            \"- Identify any major disagreements or conflicting findings.\"\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Extracted info (JSON list):\\n{extracted_info}\\n\\n\"\n",
    "            \"Return JSON with keys:\\n\"\n",
    "            \"- consensus_findings: list of objects with keys 'entity', \"\n",
    "            \"'evidence_support_papers', and 'summary'\\n\"\n",
    "            \"- conflicts: list of objects with keys 'entity', 'supporting_papers', \"\n",
    "            \"'contradicting_papers', and 'notes'\\n\"\n",
    "            \"Return ONLY JSON.\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trend_parser = JsonOutputParser(pydantic_object=None)\n",
    "\n",
    "def run_trend_analysis_agent(\n",
    "    extracted_info: List[ExtractedInfo],\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Summarize consensus and conflicts across all extracted entities.\n",
    "    \"\"\"\n",
    "    chain = trend_prompt | llm_trend | trend_parser\n",
    "    return chain.invoke({\"extracted_info\": extracted_info})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6c60e47b-4713-439f-b8ab-58ee4b561762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# CHECKPOINT AGENT – DESIGN NOTES\n",
    "# ------------------------------\n",
    "# This is where I keep myself in the loop.\n",
    "#\n",
    "# From my perspective:\n",
    "# - I don't want the pipeline to run fully autonomously, especially on\n",
    "#   sensitive topics, without me confirming the intent looks right.\n",
    "# - I also may notice, after seeing consensus/conflicts, that I want to\n",
    "#   tighten or relax some criteria (e.g., date_range, population).\n",
    "#\n",
    "# Design choices:\n",
    "# - I show:\n",
    "#     - truncated consensus findings\n",
    "#     - truncated conflicts\n",
    "#     - the current ResearchIntent as JSON\n",
    "# - I can:\n",
    "#     - press Enter to approve,\n",
    "#     - 'e' / 'n' to edit fields interactively,\n",
    "#     - 'q' to stop the pipeline entirely.\n",
    "# - Any notes I leave in `notes_for_supervisor` get stored as\n",
    "#   `checkpoint_notes` to be used by the output agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7775195d-6519-4e0c-afed-c991ad0ea50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint Agent\n",
    "\n",
    "\n",
    "# This is my human in the loop step, where I/(the user) can\n",
    "# inspect and edit the ResearchIntent before the pipeline finishes.\n",
    "\n",
    "import json\n",
    "from typing import Optional, List, Union\n",
    "from core_setup import ResearchIntent\n",
    "from core_setup import ResearchState\n",
    "\n",
    "\n",
    "def review_and_edit_intent(intent: Union[ResearchIntent, dict]) -> ResearchIntent:\n",
    "    \"\"\"\n",
    "    Interactive CLI-style loop where I can review and optionally edit\n",
    "    fields of the ResearchIntent. This is basically my original\n",
    "    checkpoint function, updated to handle both dict and Pydantic.\n",
    "    \"\"\"\n",
    "    # If we got a plain dict from upstream, wrap it back into Pydantic.\n",
    "    if isinstance(intent, dict):\n",
    "        intent = ResearchIntent(**intent)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n=== Research intent (current) ===\")\n",
    "        print(json.dumps(intent.model_dump(), indent=2)[:3000])\n",
    "\n",
    "        ans = input(\n",
    "            \"Approve intent? [Enter]=yes, 'e' or 'n'=edit, 'q'=quit: \"\n",
    "        ).strip().lower()\n",
    "\n",
    "        # Approve as is\n",
    "        if ans in (\"\", \"y\", \"yes\"):\n",
    "            return intent\n",
    "\n",
    "        # Quit the entire pipeline\n",
    "        if ans in (\"q\", \"quit\"):\n",
    "            raise SystemExit(\"Stopped at research intent review.\")\n",
    "\n",
    "        # Enter edit mode\n",
    "        if ans in (\"e\", \"n\", \"no\"):\n",
    "            print(\"\\nEdit fields (press Enter to keep current value):\\n\")\n",
    "\n",
    "            def edit_field(label: str, current: str) -> str:\n",
    "                \"\"\"Helper: edit a single scalar field.\"\"\"\n",
    "                new_val = input(f\"{label} [{current}]: \").strip()\n",
    "                return new_val or current\n",
    "\n",
    "            def edit_list_field(\n",
    "                label: str,\n",
    "                current_list: Optional[List[str]],\n",
    "            ) -> Optional[List[str]]:\n",
    "                \"\"\"Helper: edit a list field via comma-separated input.\"\"\"\n",
    "                current_str = \", \".join(current_list or [])\n",
    "                new_val = input(\n",
    "                    f\"{label} (comma-separated) [{current_str}]: \"\n",
    "                ).strip()\n",
    "                if not new_val:\n",
    "                    return current_list\n",
    "                return [x.strip() for x in new_val.split(\",\") if x.strip()]\n",
    "\n",
    "            # --- my existing field edits ---\n",
    "            intent.high_level_question = edit_field(\n",
    "                \"High-level question\", intent.high_level_question\n",
    "            )\n",
    "            intent.topic = edit_field(\"Topic\", intent.topic)\n",
    "\n",
    "            if intent.population is not None:\n",
    "                intent.population = edit_field(\"Population\", intent.population)\n",
    "            else:\n",
    "                pop_new = input(\"Population [none]: \").strip()\n",
    "                intent.population = pop_new or None\n",
    "\n",
    "            intent.date_range = edit_field(\n",
    "                \"Date range (e.g., 'last 5 years')\",\n",
    "                intent.date_range or \"\",\n",
    "            )\n",
    "\n",
    "            intent.outcomes = edit_list_field(\"Outcomes\", intent.outcomes)\n",
    "            intent.study_types = edit_list_field(\"Study types\", intent.study_types)\n",
    "            intent.must_include_terms = edit_list_field(\n",
    "                \"Must include terms\", intent.must_include_terms\n",
    "            )\n",
    "            intent.exclude_terms = edit_list_field(\n",
    "                \"Exclude terms\", intent.exclude_terms\n",
    "            )\n",
    "\n",
    "            intent.notes_for_supervisor = edit_field(\n",
    "                \"Notes for supervisor\", intent.notes_for_supervisor or \"\"\n",
    "            )\n",
    "\n",
    "            intent.semantic_scholar_query = edit_field(\n",
    "                \"Semantic Scholar query\", intent.semantic_scholar_query\n",
    "            )\n",
    "\n",
    "            # I could add editing for target_entities later if I want,\n",
    "            # but for now I leave it as is.\n",
    "            continue  # loop, show updated JSON again\n",
    "\n",
    "        print(\"Didn't understand that input; please press Enter, 'e', or 'q'.\")\n",
    "\n",
    "\n",
    "def run_checkpoint_agent(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"\n",
    "    Uses my review_and_edit_intent() function to let me inspect and\n",
    "    modify the ResearchIntent, with optional context from trend analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- CHECKPOINT: review intent in light of extracted trends ---\")\n",
    "\n",
    "    if \"consensus_findings\" in state:\n",
    "        print(\"\\nTop consensus findings (truncated):\")\n",
    "        for c in state[\"consensus_findings\"][:5]:\n",
    "            print(f\"- {c.get('entity')}: {c.get('summary')}\")\n",
    "\n",
    "    if \"conflicts\" in state:\n",
    "        print(\"\\nConflicts (truncated):\")\n",
    "        for c in state[\"conflicts\"][:5]:\n",
    "            print(f\"- {c.get('entity')}: {c.get('notes')}\")\n",
    "\n",
    "    # Here I plug in my interactive function.\n",
    "    state[\"intent\"] = review_and_edit_intent(state[\"intent\"])\n",
    "\n",
    "    # I keep any notes_for_supervisor as checkpoint_notes so the\n",
    "    # output agent can incorporate them.\n",
    "    notes = getattr(state[\"intent\"], \"notes_for_supervisor\", None)\n",
    "    state[\"checkpoint_notes\"] = notes or \"\"\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "17aa07af-5b8c-4645-b111-90302404c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# OUTPUT FORMATTING – DESIGN NOTES\n",
    "# ---------------------------------\n",
    "# This is the report writer step.\n",
    "#\n",
    "# From my perspective:\n",
    "# - I want a clean Markdown summary I can read, annotate, or paste into\n",
    "#   a doc, not raw JSON or unstructured text.\n",
    "# - The model should:\n",
    "#     - explain the main findings,\n",
    "#     - show which papers support what,\n",
    "#     - highlight conflicts and limitations.\n",
    "#\n",
    "# Design choices:\n",
    "# - I give the agent:\n",
    "#     - my original user_query,\n",
    "#     - the final intent,\n",
    "#     - consensus_findings + conflicts,\n",
    "#     - top ranked papers (with reasons),\n",
    "#     - checkpoint_notes.\n",
    "# - I ask it to produce a structured report with sections:\n",
    "#     background, methods description, main findings, conflicts,\n",
    "#     limitations, and next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dda73048-1eea-4e00-aa95-2cc2f301831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Format Agent\n",
    "\n",
    "\n",
    "# This is my final scientific writer agent that turns all the\n",
    "# structured results into a Markdown report.\n",
    "\n",
    "from core_setup import ResearchState\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm_output = ChatOpenAI(model=\"gpt-5-mini\", temperature=0.2)\n",
    "\n",
    "output_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "         \"You are a scientific writer. Format the results of a literature review \"\n",
    "         \"into clear Markdown with sections, bullet lists, and tables when helpful. \"\n",
    "         \"Assume the audience has a biology background.\"),\n",
    "        (\"user\",\n",
    "         \"User query:\\n{user_query}\\n\\n\"\n",
    "         \"Research intent:\\n{intent}\\n\\n\"\n",
    "         \"Consensus findings (JSON):\\n{consensus}\\n\\n\"\n",
    "         \"Conflicts (JSON):\\n{conflicts}\\n\\n\"\n",
    "         \"Top ranked papers (JSON, maybe truncated):\\n{ranked_papers}\\n\\n\"\n",
    "         \"Human checkpoint notes:\\n{checkpoint_notes}\\n\\n\"\n",
    "         \"Please return a well-structured Markdown report including:\\n\"\n",
    "         \"- Ranked papers/score reasoning\\n\"\n",
    "         \"- Overview / background\\n\"\n",
    "         \"- Key genes/pathways/biomarkers/causes and supporting evidence\\n\"\n",
    "         \"- Areas of disagreement or weak evidence\\n\"\n",
    "         \"- Brief methods/limitations note\\n\"\n",
    "         \"- Optional suggestions for next steps in analysis.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "def run_output_format_agent(state: ResearchState) -> str:\n",
    "    \"\"\"\n",
    "    Turn the final state into a Markdown report string.\n",
    "    \"\"\"\n",
    "    chain = output_prompt | llm_output\n",
    "    md = chain.invoke({\n",
    "        \"user_query\": state.get(\"user_query\", \"\"),\n",
    "        \"intent\": state.get(\"intent\", {}),\n",
    "        \"consensus\": state.get(\"consensus_findings\", []),\n",
    "        \"conflicts\": state.get(\"conflicts\", []),\n",
    "        \"ranked_papers\": state.get(\"ranked_papers\", [])[:10],\n",
    "        \"checkpoint_notes\": state.get(\"checkpoint_notes\", \"\"),\n",
    "    }).content\n",
    "    return md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ffc96414-6018-49bd-900a-df339830a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# SUPERVISOR AGENT - DESIGN NOTES\n",
    "# -------------------------------\n",
    "# This function is the glue that runs the whole pipeline in order.\n",
    "#\n",
    "# From my perspective:\n",
    "# - I want a single call:\n",
    "#       state = run_supervisor(\"my research question\")\n",
    "#   and then I get:\n",
    "#       - debug prints at each step,\n",
    "#       - a final `state[\"formatted_output\"]` report.\n",
    "#\n",
    "# Design choices:\n",
    "# - I explicitly log each step with \"[SUP]\" so I can see where things\n",
    "#   fail or stall.\n",
    "# - I keep the ResearchState as a simple TypedDict so it's easy to print or\n",
    "#   inspect later.\n",
    "# - The supervisor is intentionally linear right now, if I later move to\n",
    "#   LangGraph, these steps will map to nodes in the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "39487c8a-a54b-43df-8522-4ff36eeb3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervisor Agent\n",
    "\n",
    "\n",
    "# This is the orchestrator that calls all the other agents in order and\n",
    "# wires inputs/outputs together via the ResearchState dict.\n",
    "\n",
    "from core_setup import ResearchState, ResearchIntent\n",
    "\n",
    "def run_supervisor(user_query: str) -> ResearchState:\n",
    "    \"\"\"\n",
    "    Highlevel pipeline:\n",
    "      1. Intent agent -> structured ResearchIntent\n",
    "      2. Semantic Scholar search -> raw_papers\n",
    "      3. Filter agent -> filtered_papers\n",
    "      4. Quality ranking agent -> ranked_papers\n",
    "      5. Info extraction agent -> extracted_info\n",
    "      6. Trend analysis agent -> consensus/conflicts\n",
    "      7. Human checkpoint agent -> possibly updated intent + notes\n",
    "      8. Output formatting agent -> final Markdown report\n",
    "    \"\"\"\n",
    "    # Initialize state with the original user query.\n",
    "    state: ResearchState = {\"user_query\": user_query}\n",
    "\n",
    "    print(\"[SUP] Step 1: Intent agent\")\n",
    "    raw_intent = run_intent_agent(state[\"user_query\"])\n",
    "    if isinstance(raw_intent, dict):\n",
    "        state[\"intent\"] = ResearchIntent(**raw_intent)\n",
    "    else:\n",
    "        state[\"intent\"] = raw_intent\n",
    "\n",
    "    print(\"[SUP] Step 2: Semantic Scholar search\")\n",
    "    state[\"raw_papers\"] = semantic_scholar_search(state[\"intent\"], limit=50)\n",
    "    print(f\"[SUP] raw papers: {len(state['raw_papers'])}\")\n",
    "\n",
    "    print(\"[SUP] Step 3: Filter agent\")\n",
    "    filter_result = run_filter_agent(state[\"intent\"], state[\"raw_papers\"])\n",
    "\n",
    "    # From my perspective: the LLM might return either full paper objects\n",
    "    # or just IDs, so I normalize everything into a set of IDs here.\n",
    "    raw_kept = filter_result.get(\"kept\", [])\n",
    "\n",
    "    kept_ids = set()\n",
    "    for item in raw_kept:\n",
    "        # Case 1: the filter agent returned full paper dicts\n",
    "        if isinstance(item, dict) and \"id\" in item:\n",
    "            kept_ids.add(item[\"id\"])\n",
    "        # Case 2: the filter agent returned just paper IDs as strings\n",
    "        elif isinstance(item, str):\n",
    "            kept_ids.add(item)\n",
    "        # Anything else I just ignore for now\n",
    "\n",
    "    # Now I filter my original raw_papers list using this ID set\n",
    "    state[\"filtered_papers\"] = [\n",
    "        p for p in state[\"raw_papers\"] if p.get(\"id\") in kept_ids\n",
    "    ]\n",
    "    print(f\"[SUP] filtered papers: {len(state['filtered_papers'])}\")\n",
    "\n",
    "    print(\"[SUP] Step 4: Quality ranking\")\n",
    "    state[\"ranked_papers\"] = run_quality_ranking_agent(state[\"filtered_papers\"])[:20]\n",
    "    print(f\"[SUP] ranked papers: {len(state['ranked_papers'])}\")\n",
    "\n",
    "    print(\"[SUP] Step 5: Info extraction\")\n",
    "    state[\"extracted_info\"] = run_info_extraction_agent(\n",
    "        state[\"intent\"],\n",
    "        state[\"ranked_papers\"],\n",
    "    )\n",
    "    print(f\"[SUP] extracted entries: {len(state['extracted_info'])}\")\n",
    "\n",
    "    print(\"[SUP] Step 6: Trend analysis\")\n",
    "    trend_result = run_trend_analysis_agent(state[\"extracted_info\"])\n",
    "    state[\"consensus_findings\"] = trend_result.get(\"consensus_findings\", [])\n",
    "    state[\"conflicts\"] = trend_result.get(\"conflicts\", [])\n",
    "    print(f\"[SUP] consensus: {len(state['consensus_findings'])}, conflicts: {len(state['conflicts'])}\")\n",
    "\n",
    "    print(\"[SUP] Step 7: Checkpoint\")\n",
    "    state = run_checkpoint_agent(state)\n",
    "\n",
    "    print(\"[SUP] Step 8: Output formatting\")\n",
    "    state[\"formatted_output\"] = run_output_format_agent(state)\n",
    "\n",
    "    print(\"[SUP] Done, returning state\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3d27f2aa-61a3-4f05-ae53-7734ac215d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUP] Step 1: Intent agent\n",
      "[SUP] Step 2: Semantic Scholar search\n",
      "[DEBUG] Semantic Scholar query: 'breast cancer risk factors in young adults young adults'\n",
      "[SUP] raw papers: 50\n",
      "[SUP] Step 3: Filter agent\n",
      "[SUP] filtered papers: 15\n",
      "[SUP] Step 4: Quality ranking\n",
      "[SUP] ranked papers: 15\n",
      "[SUP] Step 5: Info extraction\n",
      "[SUP] extracted entries: 15\n",
      "[SUP] Step 6: Trend analysis\n",
      "[SUP] consensus: 6, conflicts: 3\n",
      "[SUP] Step 7: Checkpoint\n",
      "\n",
      "--- CHECKPOINT: review intent in light of extracted trends ---\n",
      "\n",
      "Top consensus findings (truncated):\n",
      "- Rising incidence and poorer outcomes in adolescents and young adults (AYAs; 15–39 years): Multiple reviews and GBD analyses report an increasing global incidence of breast cancer in AYAs with evidence of worse disease-free and overall survival compared with older adults. Regional predictions and GBD estimates (cases, deaths, DALYs) support a rising burden in this age group.\n",
      "- Dietary factors (higher red meat intake increases risk; higher plant-based intake and dietary fiber are protective / lower risk): Systematic reviews and GBD comparative risk assessments converge on diet as a modifiable contributor to AYA breast cancer burden. Observational syntheses link higher red meat intake to increased risk and higher plant-food/fiber intake to lower risk; GBD attributes a nontrivial fraction of DALYs to dietary risks, and country-level analyses (China) identify red meat as a leading dietary contributor.\n",
      "- Physical activity (higher activity protective; physical inactivity associated with higher incidence): Reviews and ecological modeling consistently identify higher physical activity as associated with decreased risk of AYA breast cancer, while physical inactivity appears as a risk correlate in population-level models.\n",
      "- Tobacco smoking as a contributor to AYA breast cancer burden: GBD burden estimates attribute a measurable portion of DALYs to tobacco smoking, and ecological analyses identify smoking as a risk factor associated with female breast cancer incidence in younger age groups.\n",
      "- Inherited genetic risk (heritable predisposition) and use of risk models for AYAs: Reviews emphasize that inherited genetic predisposition contributes importantly to breast cancer in AYAs and note the role of genetic testing and risk models to quantify lifetime risk and guide cascade testing. However, papers focusing on specific populations (e.g., transgender individuals) highlight that empirical data on some groups remain limited.\n",
      "\n",
      "Conflicts (truncated):\n",
      "- Body mass index (BMI) / obesity as a risk factor for AYA breast cancer: Findings are inconsistent or context-dependent. One country-level GBD analysis highlighted high BMI as an important contributor to gynecologic cancer DALYs (but not as the top contributor to breast cancer DALYs in China, where red meat ranked higher). AYA-focused reviews describe associations between obesity and AYA breast cancer as less straightforward/uncertain. Overall, evidence about BMI's role specifically for AYA breast cancer is mixed and may vary by population and cancer subtype.\n",
      "- Applicability and evidence base for genetic risk estimates in transgender (TG) and nonbinary (NB) AYAs: General AYA literature emphasizes inherited genetic risk and models for risk assessment, but reviews focused on TG/NB populations report very limited direct evidence about BRCA1/2 prevalence and penetrance in these groups and call for caution in applying typical risk estimates. This represents a gap rather than a strict contradiction: genetic risk is important overall, but its quantification and counseling in TG/NB AYAs lack robust data.\n",
      "- Air pollution (ambient ozone, PM2.5) as a risk factor for younger-onset breast cancer: An ecological analysis found positive associations between ambient ozone/PM2.5 and breast cancer incidence in younger women. However, global burden studies and dietary/environment-focused analyses did not attribute a notable fraction of AYA breast cancer burden to air pollution. This discrepancy likely reflects differences in study design (ecological vs. global burden attribution), limited direct evidence, and the need for further individual-level studies to confirm causality.\n",
      "\n",
      "=== Research intent (current) ===\n",
      "{\n",
      "  \"high_level_question\": \"What are the leading risk factors for developing breast cancer in young adults?\",\n",
      "  \"topic\": \"breast cancer risk factors in young adults\",\n",
      "  \"population\": \"young adults\",\n",
      "  \"date_range\": \"\",\n",
      "  \"outcomes\": [\n",
      "    \"leading risk factors associated with breast cancer in young adults\",\n",
      "    \"relative risks/odds ratios or hazard ratios for those factors\",\n",
      "    \"population attributable risk where available\",\n",
      "    \"incidence of breast cancer in young adult age groups\",\n",
      "    \"age at diagnosis distribution among young adults\"\n",
      "  ],\n",
      "  \"study_types\": [\n",
      "    \"cohort studies\",\n",
      "    \"case-control studies\",\n",
      "    \"cross-sectional studies\",\n",
      "    \"systematic reviews\",\n",
      "    \"meta-analyses\"\n",
      "  ],\n",
      "  \"must_include_terms\": [],\n",
      "  \"exclude_terms\": [],\n",
      "  \"notes_for_supervisor\": \"User asked broadly about 'young adults' without specifying an age range; clarify or adopt a working definition (e.g., <40 years) when designing searches. Prioritize high-quality evidence (systematic reviews/meta-analyses, large cohort and case-control studies) and more recent literature. Key domains to cover: genetic predisposition (e.g., hereditary breast cancer syndromes), family history, reproductive factors (age at menarche, parity, breastfeeding), hormonal exposures (OCs, HRT), lifestyle (alcohol, BMI, physical activity), prior chest radiation, benign breast disease, and sociodemographic factors (race/ethnicity). Search across major biomedical databases (PubMed/MEDLINE, Embase, Web of Science) and include keywords for both 'young adult' and specific age cutoffs in sensitivity analyses. If feasible, report effect sizes (RR/OR/HR) and certainty/quality of evidence.\",\n",
      "  \"semantic_scholar_query\": \"\",\n",
      "  \"depth\": \"medium\",\n",
      "  \"target_entities\": {}\n",
      "}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Approve intent? [Enter]=yes, 'e' or 'n'=edit, 'q'=quit:  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUP] Step 8: Output formatting\n",
      "[SUP] Done, returning state\n"
     ]
    }
   ],
   "source": [
    "# Here I'm actually running the whole pipeline once with a test query.\n",
    "# I can change this string to explore different topics.\n",
    "\n",
    "state = run_supervisor(\"Leading risk of breast cancer in young adults\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "aaf71d1c-b66f-48b9-9b3c-e9af5aba6856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is a concise literature‑review-style summary addressing the question:\n",
       "What are the leading risk factors for developing breast cancer in young adults?  \n",
       "I adopt a working definition used by many AYA studies: adolescents and young adults (AYA) = ages 15–39 years (I note where studies use other cutoffs). The review prioritizes systematic reviews/meta‑analyses, large cohort and case‑control studies, and recent Global Burden of Disease (GBD) estimates when available.\n",
       "\n",
       "Executive summary\n",
       "- Breast cancer incidence in AYAs (15–39) is rising in many regions and AYAs often experience worse disease‑free and overall survival than older adults.\n",
       "- Major contributors to AYA breast cancer risk include inherited genetic syndromes (BRCA1/2, TP53, etc.), family history, reproductive/hormonal factors, prior therapeutic chest radiation, certain lifestyle factors (diet — high red meat; low plant/fiber; physical inactivity; alcohol; tobacco), and some benign breast diseases.\n",
       "- Evidence strength varies: genetic and prior chest radiation risks are large and well supported; lifestyle and BMI associations for AYAs are often smaller, heterogeneous, or context dependent. Population attributable fractions reported by GBD point to diet and tobacco as measurable contributors to AYA breast cancer burden.\n",
       "- Important gaps/conflicts: BMI’s role in AYA breast cancer is inconsistent across studies; ambient air pollution and evidence in transgender/nonbinary AYAs are currently limited.\n",
       "\n",
       "Ranked key publications (selection) and reasoning\n",
       "- Zheng et al., 2025 (GBD 2021 analysis of AYAs 15–39) — large, up‑to‑date global burden estimates and attributable risk fractions (dietary risks, tobacco). High relevance for incidence/trend and PAFs.\n",
       "- McVeigh et al., 2021 (“A Review of Breast Cancer Risk Factors in Adolescents and Young Adults”, Cancers) — focused AYA review covering genetic, environmental, and lifestyle risks; useful synthesis for clinical implications.\n",
       "- Cathcart‑Rake et al., 2018 (Cancer Journal) — review on modifiable risk factors in young women (physical activity, diet, BMI); helpful for modifiable exposures.\n",
       "- Zhao et al., 2022 (GBD 2019, China focus) — country‑level comparative risk assessment pointing to red meat and BMI contributions.\n",
       "- Yuan et al., 2024 (ecological study, NY State) — county‑level associations identifying ambient air pollution and other population exposures associated with younger‑onset breast cancer (hypothesis‑generating).\n",
       "- Other useful reviews: pediatric/AYA oncology overviews (2022), classic endocrine‑risk reviews (Bernstein 2004) for mechanistic context.\n",
       "\n",
       "(These papers were prioritized because they are recent, AYA‑focused, or use large, population‑level data. Limitations of individual papers are noted below.)\n",
       "\n",
       "Background / burden and incidence in young adults\n",
       "- GBD (2021, AYA 15–39): ~180,791 new breast cancer cases among AYAs globally in 2021; age‑standardized incidence rising (reported AAPC for women ~3.0%). Regional heterogeneity: higher incidence in high‑SDI regions, higher mortality in low‑SDI regions.\n",
       "- Country examples: China (GBD 2019‑derived analysis) estimated 61,038 incident FeBGC cases among AYAs in 2019 and predicted rising incidence through 2030.\n",
       "- Age distribution within AYAs: the bulk of “young adult” breast cancers cluster at the older end of the AYA range (late 20s–late 30s); true pediatric breast cancer is uncommon. AYAs as a group have a higher proportion of aggressive subtypes and worse outcomes relative to older adults.\n",
       "\n",
       "Leading risk factors — summary, evidence level, and typical effect sizes (where available)\n",
       "Below is a condensed table showing major domains, direction/relative magnitude, and evidence strength. Effect sizes are given as ranges reported in AYA‑targeted or large general studies when AYA‑specific estimates were available; many effect sizes come from mixed‑age analyses and are therefore flagged.\n",
       "\n",
       "| Risk factor (domain) | Direction / magnitude (typical effect) | Evidence level (for AYAs) | Notes / population attributable fraction (PAF) where available |\n",
       "|---|---:|---|---|\n",
       "| Inherited high‑penetrance genes (BRCA1, BRCA2, TP53, PALB2, PTEN, CHEK2, ATM) | Large increase in risk. BRCA1/2 carriers: markedly elevated lifetime risks; substantial proportion of carriers develop breast cancer before age 40. | High (genetic cohort studies, registry data, systematic reviews) | BRCA1/2 major causes of early‑onset cases. Exact age‑specific penetrance varies by study/population (see text). |\n",
       "| Family history (first‑degree relative) | Approx. 2–3× increased risk (varies with age of affected relative and number of relatives) | High (case‑control, cohort) | Family history remains an independent predictor after accounting for known mutations. |\n",
       "| Prior chest radiation (e.g., mantle RT for Hodgkin lymphoma in adolescence) | Very large. RR often several‑fold; cumulative incidence by 30–40 years dramatically increased (some studies report cumulative incidence ~10–20% by age 40 among exposed survivors). | High (cohort studies of childhood/adolescent cancer survivors) | One of the strongest modifiable iatrogenic risks for young‑onset breast cancer; screening guidelines exist for survivors. |\n",
       "| Reproductive/hormonal (early menarche, nulliparity, late first childbirth, shorter breastfeeding) | Direction consistent with adult literature: early menarche and nulliparity/late first birth → increased risk. Magnitudes modest (RRs typically 1.1–1.5 depending on comparison). | Moderate (cohort analyses; many studies include broader age ranges) | Breastfeeding protective in many studies; effect sizes modest but relevant to early‑onset risk. |\n",
       "| Oral contraceptives (OCs) / exogenous hormones | Small increased risk associated with current/recent OC use in many studies (RR ~1.1–1.3); effects decline after cessation. | Moderate (meta‑analyses, cohort studies) | Most data come from general adult cohorts; AYA‑specific data limited but concern is often emphasized because of exposure during reproductive years. |\n",
       "| Alcohol consumption | Dose‑dependent increase; typical estimates (all ages) ~7–10% increased risk per 10 g/day; for AYAs evidence consistent but limited on precise age‑specific RRs. | Moderate (meta‑analyses, cohort) | GBD attributes some breast cancer DALYs to alcohol; effect present across ages. |\n",
       "| Diet (high red meat; low plant/fiber) | Higher red meat associated with increased risk; higher plant/fiber intake protective. Effect sizes modest (RRs commonly <1.5). | Low–moderate (observational studies, meta‑analyses, GBD) | GBD AYA analyses name dietary risks as the largest modifiable contributor to DALYs in some regions (e.g., China). |\n",
       "| Physical activity | Protective (higher activity associated with ≈10–30% lower risk in many studies). | Moderate (cohort/meta‑analysis) | Protective effect reported in AYAs and broader adult cohorts. |\n",
       "| Tobacco smoking | Small but measurable increased risk; GBD attributes a portion of DALYs to tobacco in AYAs. | Low–moderate (ecological/GBD, cohort evidence mixed) | Smoking exposure prior to first full‑term pregnancy may be particularly relevant biologically. |\n",
       "| Body mass index (BMI) / obesity | Mixed/inconsistent for AYAs: in general adult literature, higher BMI is associated with higher postmenopausal breast cancer risk but sometimes with lower premenopausal risk. AYA‑specific findings are heterogeneous. | Low–conflicting (GBD/country analyses vs. AYA‑focused reviews) | Some GBD/country studies list high BMI as an important contributor; other AYA reviews find unclear or subtype‑dependent associations. |\n",
       "| Benign proliferative breast disease | Increased risk (relative risk varies by histology; proliferative lesions with atypia confer higher risk). | Moderate (cohort/case‑control studies) | Often identified clinically at younger ages and known to increase future risk. |\n",
       "| Socioeconomic / race/ethnicity | Heterogeneous effects: higher incidence in high‑SDI regions but higher mortality in low‑SDI regions. Black women more likely to present with aggressive subtypes and higher mortality at younger ages. | Moderate (population studies, registry data) | Reflects both exposure distributions and access to care. |\n",
       "| Ambient air pollution (PM2.5, ozone) | Emerging/ecological evidence suggests positive associations in some studies; individual‑level causality not established. | Low (ecological; hypothesis‑generating) | Needs confirmation in individual‑level analytic studies. |\n",
       "\n",
       "Key genes / pathways / biomarkers and supporting evidence\n",
       "- DNA repair and homologous recombination pathway: BRCA1 and BRCA2\n",
       "  - BRCA1 is strongly associated with early‑onset, triple‑negative phenotype in many carriers; BRCA2 also confers early risk but the phenotype distribution differs.\n",
       "  - Age‑specific penetrance varies by study/population. Many BRCA1/2 carriers accumulate a substantial proportion of lifetime breast cancer risk before age 40.\n",
       "- TP53 (Li‑Fraumeni syndrome)\n",
       "  - Strongly increases risk for early‑onset breast cancer (often premenopausal); carriers are at markedly elevated risk across childhood/young adulthood.\n",
       "- PALB2, CHEK2, ATM, PTEN (Cowden)\n",
       "  - Moderate‑ to high‑penetrance genes that increase early‑onset risk in carriers (magnitude varies).\n",
       "- Hormone receptor and intrinsic subtypes\n",
       "  - Younger women have a higher relative prevalence of biologically aggressive subtypes (e.g., triple‑negative and HER2‑positive) compared with older women in many studies; this influences prognosis and likely reflects different etiologies.\n",
       "- Biomarkers used clinically for risk stratification\n",
       "  - Multi‑gene panels, polygenic risk scores (PRS) and family history models are increasingly applied to quantify risk and guide surveillance; evidence supports their utility, but calibration for AYAs and for non‑European populations is variable.\n",
       "\n",
       "Areas of disagreement, uncertainty, or weak evidence\n",
       "- BMI / obesity: evidence for BMI’s effect on breast cancer risk in AYAs is inconsistent. Adult literature shows opposite directions by menopausal status, but AYA‑specific studies vary by geography, subtype, and study design. Some GBD/country analyses highlight BMI as an important contributor to cancer burden, while AYA reviews find the association less straightforward.\n",
       "- Air pollution: ecological analyses (e.g., NY State county data) reported positive associations between PM2.5/ozone and younger‑onset breast cancer, but global burden studies have not attributed a substantial fraction of AYA breast cancer to air pollution. Individual‑level causal evidence remains limited.\n",
       "- Hormonal contraceptives: many adult studies report small increased risk with current/recent use; AYA‑specific effect sizes are modest and confounded by indication and reproductive behavior. Overall clinical consensus is cautious but not prohibitive.\n",
       "- Transgender and nonbinary AYAs: empirical data on genotype/penetrance and breast cancer risks (e.g., after gender‑affirming hormones or surgeries) are limited; current risk models may not directly generalize.\n",
       "- Population attributable fractions (PAFs): GBD provides PAFs for categories such as dietary risk and tobacco for AYAs in some analyses, but these are modelled estimates that depend on exposure prevalence and assumed relative risks and may not capture regional or subtype differences.\n",
       "\n",
       "Methods / limitations of the evidence base (brief)\n",
       "- Heterogeneous age definitions: many studies use different “young” cutoffs (e.g., <35, <40, 15–39), making synthesis imperfect.\n",
       "- Many effect sizes come from mixed‑age cohort/meta‑analyses; truly AYA‑specific prospective data are fewer.\n",
       "- GBD and ecological studies provide useful population‑level PAFs but rely on modelling assumptions and can differ from individual‑level estimates.\n",
       "- Confounding and reverse causation risk in observational studies of lifestyle factors (e.g., smoking, alcohol, diet). Measurement error in self‑reported exposures.\n",
       "- Genetic penetrance estimates vary by ancestry and study; PRS and gene panels are still being calibrated across populations.\n",
       "- Subtype heterogeneity (ER/PR/HER2) is an important modifier but is not always reported or stratified in studies.\n",
       "\n",
       "Selected quantitative estimates (examples, approximate ranges and caveats)\n",
       "- Global AYA burden: GBD 2021 (AYAs 15–39) ≈ 180,791 incident cases (2021); AAPC incidence in women ~ +3.0% (1990–2021).\n",
       "- GBD attributable fractions (AYAs, selected): dietary risks (example: 10.5% of DALYs in one GBD AYA analysis), tobacco ~2.0% of DALYs; high fasting plasma glucose ~1.6% (Zheng et al., 2025). These are modelled PAFs and vary by region.\n",
       "- Prior chest radiation: cohort studies of childhood/adolescent Hodgkin lymphoma survivors show sharply increased breast cancer risks with RRs often several‑fold; cumulative incidence by age 40 among exposed survivors can be on the order of 10–20% in some series (varies by dose and field).\n",
       "- BRCA1/2: carriers have markedly elevated lifetime risks; many carriers develop breast cancer at younger ages. Age‑specific penetrance estimates vary by paper and population; reported lifetime risks to age 70 commonly cited around 45–70% for BRCA1 and 40–60% for BRCA2 in older literature — a substantial fraction of those events occur before age 40 in many carriers. (Use mutation‑specific cohort data for precise counseling.)\n",
       "- OCPs: pooled adult estimates suggest small increased risk (e.g., RR ≈1.1–1.3 for current/recent users) that wanes over time since stopping; AYA‑specific RRs are similar but data are limited.\n",
       "- Alcohol: pooled adult estimates ~7–10% increased risk per 10 g/day alcohol intake; AYA‑specific incremental risk not well quantified separately.\n",
       "\n",
       "Practical implications for clinicians / public health\n",
       "- Genetic testing and early surveillance: identify individuals with family history or early cancers for genetic testing (BRCA1/2, TP53, etc.) because these confer high absolute risks at young ages and change management (enhanced surveillance, risk‑reducing options).\n",
       "- Prior chest radiation survivors require early screening (MRI ± mammography) because of high cumulative risk.\n",
       "- Population prevention: promote modifiable protective behaviors (physical activity, healthy diet, limiting alcohol, breastfeeding where possible) while acknowledging effect sizes are modest individually.\n",
       "- Equity: targeted efforts are needed because incidence and mortality patterns differ by SDI, race/ethnicity, and access to care.\n",
       "\n",
       "Suggestions for next steps in analysis or research (if you want to go deeper)\n",
       "1. Define the exact age range for “young adults” for your analysis (recommended: 15–39 for comparability with AYA literature, or <40 if simpler).\n",
       "2. Conduct a focused systematic search (PubMed/MEDLINE, Embase, Web of Science) limited to AYA age cutoffs and these study types: systematic reviews/meta‑analyses, large cohorts, population registries, and case‑control studies. Use search terms: “breast cancer” AND (“young adult” OR adolescent OR “early onset” OR “<40” OR “15–39”) plus risk factor terms (BRCA, radiation, parity, OCP, BMI, alcohol, diet, smoking, physical activity, PM2.5).\n",
       "3. Extract age‑specific effect sizes (RR/OR/HR) stratified by subtype (ER+/ER−/HER2+/TNBC) where available. Perform meta‑analysis if sufficient homogeneous studies exist.\n",
       "4. Estimate population attributable fractions for your target population using measured exposure prevalences and pooled RRs (with sensitivity analyses).\n",
       "5. Stratify by geography/SDI and race/ethnicity where possible, because both exposure prevalence and baseline incidence differ substantially.\n",
       "6. If focusing on genetic risk, compile gene‑specific age‑specific penetrance estimates by ancestry and assess calibration of PRS in AYAs.\n",
       "\n",
       "Concluding remark\n",
       "The strongest, most consistently supported drivers of breast cancer in young adults are inherited high‑penetrance genetic variants (BRCA1/2, TP53, etc.), family history, and prior chest radiation; these confer substantial age‑specific risks and have direct clinical implications (testing, surveillance, risk‑reducing interventions). Lifestyle and environmental contributors (diet, alcohol, physical activity, tobacco) are plausible and contribute at the population level (GBD PAFs), but effect sizes for individual risk and the evidence for some exposures (BMI, air pollution) in AYAs are heterogeneous and require more AYA‑focused, individual‑level studies.\n",
       "\n",
       "If you want, I can:\n",
       "- run a structured literature search and extract effect sizes (RR/OR/HR) for a specified age cutoff (e.g., 15–39 or <40),\n",
       "- prepare an evidence table with study designs, sample sizes, and numeric effect estimates,\n",
       "- or focus on one domain (e.g., genetic/heritability vs modifiable lifestyle risks) and produce a targeted, referenced summary. Which do you prefer?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally, I render the Markdown report that the output agent produced.\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(state[\"formatted_output\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0f678-6bef-4221-a61a-94a576edd2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
